{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437cf5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anirudh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/anirudh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 S00789.txt\n",
      "200 T00740.txt\n",
      "300 C00106.txt\n",
      "400 L00071.txt\n",
      "500 C00585.txt\n",
      "600 S00988.txt\n",
      "700 T00612.txt\n",
      "800 D00813.txt\n",
      "900 C00061.txt\n",
      "1000 S00920.txt\n",
      "1100 P04035.txt\n",
      "1200 S00511.txt\n",
      "1300 P05034.txt\n",
      "1400 P_1375.txt\n",
      "1500 C00981.txt\n",
      "1600 P_569.txt\n",
      "1700 M00189.txt\n",
      "1800 C00273.txt\n",
      "1900 D00369.txt\n",
      "2000 L00438.txt\n",
      "2100 P_1174.txt\n",
      "2200 R00170.txt\n",
      "2300 M00089.txt\n",
      "2400 C00154.txt\n",
      "2500 T00760.txt\n",
      "2600 P_2079.txt\n",
      "2700 L00303.txt\n",
      "2800 P_824.txt\n",
      "2900 P_23.txt\n",
      "3000 R00124.txt\n",
      "3100 D00657.txt\n",
      "3200 D00697.txt\n",
      "3300 M00150.txt\n",
      "3400 D00515.txt\n",
      "3500 P_562.txt\n",
      "3600 C00977.txt\n",
      "3700 C00616.txt\n",
      "3800 C00624.txt\n",
      "3900 D00736.txt\n",
      "4000 T00682.txt\n",
      "4100 C00847.txt\n",
      "4200 C00803.txt\n",
      "4300 S00728.txt\n",
      "4400 C00770.txt\n",
      "4500 L00308.txt\n",
      "4600 P_2063.txt\n",
      "4700 P_1602.txt\n",
      "4800 P_1298.txt\n",
      "4900 P_1780.txt\n",
      "5000 C00539.txt\n",
      "5100 P_1400.txt\n",
      "5200 D00301.txt\n",
      "5300 R00414.txt\n",
      "5400 D00478.txt\n",
      "5500 D00959.txt\n",
      "5600 R00152.txt\n",
      "5700 P_157.txt\n",
      "5800 P03098.txt\n",
      "5900 T00903.txt\n",
      "6000 L00080.txt\n",
      "6100 L00496.txt\n",
      "6200 L00400.txt\n",
      "6300 P_1589.txt\n",
      "6400 C00890.txt\n",
      "6500 T00404.txt\n",
      "6600 T01115.txt\n",
      "6700 S00295.txt\n",
      "6800 S00255.txt\n",
      "6900 D00398.txt\n",
      "7000 P01004.txt\n",
      "7100 D00149.txt\n",
      "7200 D00504.txt\n",
      "7300 T00039.txt\n",
      "7400 L00029.txt\n",
      "7500 T00647.txt\n",
      "7600 P_1928.txt\n",
      "7700 P_1976.txt\n",
      "7800 P_1162.txt\n",
      "7900 P_1924.txt\n",
      "8000 P_2090.txt\n",
      "8100 C00887.txt\n",
      "8200 P_51.txt\n",
      "8300 P02097.txt\n",
      "8400 S00832.txt\n",
      "8500 S00432.txt\n",
      "8600 L00026.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import operator\n",
    "import pickle\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#Global variables\n",
    "files_path = 'english-corpora/' #Directory in which corpus files are present\n",
    "files = os.listdir(files_path)  \n",
    "\n",
    "#posting list, key=token,value=list of doc_id's\n",
    "boolean_posting_list = {}\n",
    "#posting list, key=token,value=list of doc_id's with frequency\n",
    "tf_posting_list = {}\n",
    "#list, value=(token, freq in that document index)\n",
    "document_tf_list = []\n",
    "#dict,no of times a token appears in all documents, useful in idf calculation\n",
    "token_document_frequency = {}\n",
    "#stores all unique tokens\n",
    "vocabulary = set()\n",
    "#key=filename,value = index(0,1,2)\n",
    "filename_index_dict = {}\n",
    "#key=index,value=filename\n",
    "filename_index_inv_dict = {}\n",
    "#No of tokens in each doc,including non unique tokens\n",
    "doc_len = []\n",
    "#key=token,value=list of (doc_id,tfidf value)\n",
    "tf_idf_list = {}\n",
    "\n",
    "\n",
    "#It is of the form {token1:[[doc1,doc2,doc5] token2:[doc1,doc10]}\n",
    "def boolean_retrieval_system(token_list:[], doc_id: int):\n",
    "    token_set = set(token_list)\n",
    "    for token in token_set:\n",
    "        if(not (token in boolean_posting_list)):\n",
    "            boolean_posting_list[token] = []\n",
    "        boolean_posting_list[token].append(doc_id)  \n",
    "        \n",
    "#It is of the form {token1:[(doc1,cnt1) (doc2,cnt2)], token2:[(doc1,cnt1)]}\n",
    "#Here cnt is no of times token1 appears in doc1\n",
    "def tf_retrieval_system(token_list:[], doc_id: int):\n",
    "    tf_map = {}\n",
    "    for token in token_list:\n",
    "        if(token in tf_map):\n",
    "            tf_map[token] += 1\n",
    "        else:\n",
    "            tf_map[token] = 1\n",
    "       \n",
    "    document_tf_list.append({})        \n",
    "    for token in tf_map:\n",
    "        if(not (token in tf_posting_list)):\n",
    "            tf_posting_list[token] = []\n",
    "        tf_posting_list[token].append((doc_id, tf_map[token]))\n",
    "        if(token in token_document_frequency):\n",
    "            token_document_frequency[token] += 1\n",
    "        else:\n",
    "            token_document_frequency[token] = 1\n",
    "        \n",
    "        document_tf_list[doc_id][token] = tf_map[token]\n",
    "def clean_text(data: str)->str:\n",
    "    sub_space   = '[. \\- ,: /; \\[\\]\\(\\)=_]' #substitutes the special chars with space\n",
    "    data = data.replace(\"\\\\\", \" \")    #replaces / with space\n",
    "    data = re.sub(sub_space,' ',data) \n",
    "    data = data.replace('\"',\"\") \n",
    "    data = data.replace(\"'\",\"\")  #replaces quotes with nothing\n",
    "    cleaned_data = data.encode('ascii',errors='ignore').decode() #ignores non-ascii chars\n",
    "    return cleaned_data\n",
    "    \n",
    "\n",
    "def tokenize_text(data: str):\n",
    "    word_data = word_tokenize(data)   #word tokenizer lib\n",
    "    token_list = []\n",
    "    for word in word_data:\n",
    "        #if(len(word)>1 and (word not in Stopwords)):\n",
    "        if(len(word)>1 or (word in ['a','i'])):                         #tokenizing text.Including stop words\n",
    "            token_list.append(word.lower())\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def stem_text(token_data: []):\n",
    "    stemmed_tokens = []\n",
    "    for token in token_data:\n",
    "        stemmed_tokens.append(ps.stem(token))  #porter stemmer\n",
    "    return stemmed_tokens\n",
    "        \n",
    "\n",
    "def insert_unique_tokens(tokens: []):\n",
    "    for token in tokens:\n",
    "        vocabulary.add(token)  #all the unique tokens\n",
    "    return vocabulary\n",
    "\n",
    "def preprocess(data: str):\n",
    "    cleaned_data   = clean_text(data)\n",
    "    token_data     = tokenize_text(cleaned_data)\n",
    "    stemmed_tokens = stem_text(token_data)\n",
    "    return stemmed_tokens\n",
    "    \n",
    "doc_cnt = 0\n",
    "for file in files:\n",
    "    f = open(os.path.join(files_path, file),'r',encoding=\"utf8\")\n",
    "    \n",
    "    data = f.read().replace('\\n', ' ')\n",
    "    \n",
    "    filename_index_dict[file] = doc_cnt            #indexing filename with numbers.Eg.C0001->0,C0002->1...\n",
    "    filename_index_inv_dict[doc_cnt] = file\n",
    "    \n",
    "    tokens = preprocess(data)\n",
    "    insert_unique_tokens(tokens)                  #create vocabulary\n",
    "    boolean_retrieval_system(tokens, doc_cnt)     #create boolean posting list\n",
    "    tf_retrieval_system(tokens, doc_cnt)          #create term frequency posting list\n",
    "    doc_len.append(len(tokens))                   #stores no of tokens\n",
    "    f.close()\n",
    "    doc_cnt+=1\n",
    "    if(doc_cnt%100==0):\n",
    "        print(doc_cnt, file)                      #debugging\n",
    "    \n",
    "        \n",
    "#Calculates idf \n",
    "def calc_idf_tfidfalgo(token: str)->int:\n",
    "    if(token in token_document_frequency):\n",
    "        return math.log(doc_cnt/(token_document_frequency[token]+1))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "    \n",
    "#calculates tf\n",
    "def calc_tfidf(freq:int, idf: int)->int:\n",
    "    if(freq==0):\n",
    "        return 0\n",
    "    else:\n",
    "        return (1+math.log(freq))*idf\n",
    "    \n",
    "    \n",
    "normalized_doc_len = [0]*doc_cnt #stores norm of tfidf vectors\n",
    "#converts tf-idf scores to document vectors\n",
    "#Eg. doc1[token1: (term1, tfidf1),token2: (term2, tfidf2)]\n",
    "def vectorize_tfidf():\n",
    "    tfidf_doc_score = [None]*doc_cnt\n",
    "    \n",
    "    for token in tf_idf_list:\n",
    "        for doc_tuple in tf_idf_list[token]:\n",
    "            doc_id, score = doc_tuple\n",
    "            if(tfidf_doc_score[doc_id]==None):\n",
    "                tfidf_doc_score[doc_id] = {}\n",
    "            tfidf_doc_score[doc_id][token] = score\n",
    "            \n",
    "            normalized_doc_len[doc_id] += (score*score)\n",
    "           # print('doc_id is ',doc_id)\n",
    "        \n",
    "    for doc_id in range(doc_cnt):\n",
    "        for token in tfidf_doc_score[doc_id]:\n",
    "            tfidf_doc_score[doc_id][token] = tfidf_doc_score[doc_id][token]\n",
    "    return tfidf_doc_score\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "#calculates tf-idf\n",
    "def tfidf_system():\n",
    "    for token in tf_posting_list:\n",
    "        freq_list = tf_posting_list[token]\n",
    "        tf_idf_list[token] = []\n",
    "        idf_tfidf_algo   = calc_idf_tfidfalgo(token)\n",
    "        for freq_tuple in freq_list:\n",
    "            doc_id, freq = freq_tuple\n",
    "            tf_idf_list[token].append((doc_id, calc_tfidf(freq, idf_tfidf_algo)))\n",
    "\n",
    "tfidf_system()\n",
    "\n",
    "tfidf_doc_vector_list =  vectorize_tfidf()  \n",
    "\n",
    "#store all the above files in a dump for faster query processing\n",
    "#It avoids building global variables everytime a query is run\n",
    "vocabulary_file               = open(\"vocabulary.txt\", \"wb\")\n",
    "boolean_posting_list_file     = open(\"boolean_posting_list.txt\", \"wb\")\n",
    "token_document_frequency_file = open(\"token_document_frequency.txt\", \"wb\")\n",
    "tfidf_doc_vector_list_file    = open(\"tfidf_doc_vector_list.txt\", \"wb\")\n",
    "normalized_doc_len_file       = open(\"normalized_doc_len.txt\", \"wb\")\n",
    "doc_len_file                  = open(\"doc_len.txt\", \"wb\")\n",
    "filename_index_inv_dict_file  = open(\"filename_index_inv_dict.txt\", \"wb\")\n",
    "document_tf_list_file         = open(\"document_tf_list.txt\",\"wb\")\n",
    "\n",
    "\n",
    "pickle.dump(vocabulary, vocabulary_file)\n",
    "pickle.dump(boolean_posting_list, boolean_posting_list_file)\n",
    "pickle.dump(token_document_frequency, token_document_frequency_file)\n",
    "pickle.dump(tfidf_doc_vector_list, tfidf_doc_vector_list_file)\n",
    "pickle.dump(normalized_doc_len, normalized_doc_len_file)\n",
    "pickle.dump(doc_len, doc_len_file)\n",
    "pickle.dump(filename_index_inv_dict, filename_index_inv_dict_file)\n",
    "pickle.dump(document_tf_list, document_tf_list_file)\n",
    "\n",
    "vocabulary_file.close()\n",
    "boolean_posting_list_file.close()\n",
    "token_document_frequency_file.close()\n",
    "tfidf_doc_vector_list_file.close()\n",
    "normalized_doc_len_file.close()\n",
    "doc_len_file.close()\n",
    "filename_index_inv_dict_file.close()\n",
    "document_tf_list_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
