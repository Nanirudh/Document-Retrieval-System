{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "732bb749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/anirudh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/anirudh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import operator\n",
    "import pickle\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "import sys\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ec0bed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total no. of documents\n",
    "doc_cnt = 101\n",
    "\n",
    "\n",
    "\n",
    "#unique tokens\n",
    "with open('vocabulary.txt', 'rb') as handle:\n",
    "    data                            = handle.read()\n",
    "    vocabulary                      = pickle.loads(data)\n",
    "\n",
    "#posting list, key=token,value=list of doc_id's    \n",
    "with open('boolean_posting_list.txt', 'rb') as handle:\n",
    "    data                            = handle.read()\n",
    "    boolean_posting_list            = pickle.loads(data)\n",
    "#dict,no of times a token appears in all documents, useful in idf calculation    \n",
    "with open('token_document_frequency.txt', 'rb') as handle:\n",
    "    data                            = handle.read()\n",
    "    token_document_frequency        = pickle.loads(data)\n",
    "#document vector of tf-idf values    \n",
    "with open('tfidf_doc_vector_list.txt', 'rb') as handle:\n",
    "    data                            = handle.read()\n",
    "    tfidf_doc_vector_list           = pickle.loads(data)\n",
    "#stores norm of tfidf vectors size is doc_cnt    \n",
    "with open('normalized_doc_len.txt', 'rb') as handle:\n",
    "    data                            = handle.read()\n",
    "    normalized_doc_len              = pickle.loads(data)\n",
    "    \n",
    "\n",
    "#no of tokens in a document    \n",
    "with open('doc_len.txt', 'rb') as handle:\n",
    "    data                            = handle.read()\n",
    "    doc_len                         = pickle.loads(data)\n",
    "    \n",
    "#key=index,value=filename\n",
    "with open('filename_index_inv_dict.txt', 'rb') as handle:\n",
    "    data                            = handle.read()\n",
    "    filename_index_inv_dict         = pickle.loads(data)\n",
    "    \n",
    "#list, value=(token, freq in that document index)\n",
    "with open('document_tf_list.txt', 'rb') as handle:\n",
    "    data                            = handle.read()\n",
    "    document_tf_list                = pickle.loads(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b1bf731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_retrieval_query(query):\n",
    "    tokens         = preprocess_query(query)\n",
    "    similar_docs   = {}\n",
    "    #boolean retrieval does not consider term freq\n",
    "    tokens         = set(tokens) \n",
    "    \n",
    "    #initializing dictionary.Initially all documents a\n",
    "    for doc_id in range(doc_cnt):\n",
    "        similar_docs[doc_id] = 0\n",
    "    \n",
    "    \n",
    "    for token in tokens:\n",
    "        #if token is present in posting list\n",
    "        if(token in boolean_posting_list):\n",
    "            for doc_id in boolean_posting_list[token]:\n",
    "                if(doc_id in similar_docs):\n",
    "                    similar_docs[doc_id] +=1\n",
    "                else:\n",
    "                    similar_docs[doc_id]  =1\n",
    "        else: #token not matched with any document.So we consider substring match.Modification with boolean retrieval\n",
    "            for posting_token in boolean_posting_list:\n",
    "                if(posting_token.find(token)!=-1):\n",
    "                    for doc_id in boolean_posting_list[posting_token]:\n",
    "                        similar_docs[doc_id]+=1\n",
    "            \n",
    "            \n",
    "    similar_docs = dict( sorted(similar_docs.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    ranked_documents = rank_documents(similar_docs)\n",
    "    return ranked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789aa28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculates idf\n",
    "def calc_idf_tfidfalgo(token: str)->int:\n",
    "    if(token in token_document_frequency):\n",
    "        return math.log(doc_cnt/(token_document_frequency[token]+1))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#calculates tf\n",
    "def calc_tfidf(freq:int, idf: int)->int:\n",
    "    if(freq==0):\n",
    "        return 0\n",
    "    else:\n",
    "        return (1+math.log(freq))*idf\n",
    "\n",
    "\n",
    "def tfidf_query(query)->[]:\n",
    "    tokens      = preprocess_query(query)\n",
    "    query_tfidf = {}\n",
    "    tf_freq_map = {}\n",
    "    query_vector_sqrt_val = 0\n",
    "    \n",
    "    #converting query into tfidf vector to find cosine similarity\n",
    "    for token in tokens:\n",
    "        if(not(token in tf_freq_map)):\n",
    "            tf_freq_map[token]=1\n",
    "        else:\n",
    "            tf_freq_map[token]+=1\n",
    "    for token in tf_freq_map:\n",
    "        idf = calc_idf_tfidfalgo(token)\n",
    "        query_tfidf[token] = calc_tfidf(tf_freq_map[token], idf)\n",
    "        query_vector_sqrt_val += (query_tfidf[token]*query_tfidf[token])\n",
    "        \n",
    "    query_vector_sqrt_val = math.sqrt(query_vector_sqrt_val)\n",
    "        \n",
    "    #cosine similarity\n",
    "    doc_index = 0\n",
    "    doc_similarity_dict = {}\n",
    "    for tfidf_doc_vector_dict in tfidf_doc_vector_list:\n",
    "        similarity_val = 0\n",
    "        for query_token in query_tfidf:\n",
    "            if(query_token in tfidf_doc_vector_dict):\n",
    "                similarity_val += (query_tfidf[query_token]*tfidf_doc_vector_dict[query_token])\n",
    "                similarity_val /= math.sqrt(normalized_doc_len[doc_index])\n",
    "                similarity_val /= query_vector_sqrt_val\n",
    "        doc_similarity_dict[doc_index] = similarity_val\n",
    "        doc_index+=1\n",
    "    #greater cos(theta) value has greater similarity\n",
    "    doc_similarity_dict = dict( sorted(doc_similarity_dict.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    ranked_documents = rank_documents(doc_similarity_dict)\n",
    "    return ranked_documents\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d734558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idf of bm25 algorithm\n",
    "def calc_idf_bm25algo(token: str)->int:\n",
    "    if(token in token_document_frequency):\n",
    "        return math.log(1+((doc_cnt-token_document_frequency[token]+0.5)/(token_document_frequency[token] + 0.5)))\n",
    "    else:\n",
    "        return 0\n",
    "#tf of bm25 algorithm\n",
    "def calc_bm25_tf(freq:int, idf:int, doc_id:int)->int:\n",
    "    k = 1.5\n",
    "    b = 0.75\n",
    "    avg_doc_len = sum(doc_len)/len(doc_len)\n",
    "    return ((freq*(k+1))/(freq + k*(1-b+b*(doc_len[doc_id]/avg_doc_len))))\n",
    "\n",
    "\n",
    "\n",
    "def bm25_query(query)->[]:\n",
    "    tokens                = preprocess_query(query)\n",
    "    similarity_score_dict = {}\n",
    "    for doc_id in range(doc_cnt):\n",
    "        score = 0\n",
    "        for token in tokens:\n",
    "            if(token in document_tf_list[doc_id]):\n",
    "                freq = document_tf_list[doc_id][token]\n",
    "            else:\n",
    "                freq = 0\n",
    "            bm25_idf = calc_idf_bm25algo(token)\n",
    "            bm25_tf  = calc_bm25_tf(freq, bm25_idf, doc_id)\n",
    "            score   += (bm25_idf*bm25_tf) #calculates similarity score for each document\n",
    "        similarity_score_dict[doc_id] = score\n",
    "    similarity_score_dict = dict( sorted(similarity_score_dict.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    ranked_documents = rank_documents(similarity_score_dict)\n",
    "    return ranked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767affab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(doc_similarity_dict)->[]:\n",
    "    ranked_list = []\n",
    "    for key in enumerate(doc_similarity_dict):\n",
    "        ranked_list.append(filename_index_inv_dict[key[1]])\n",
    "        #selecting top 5 documents 0,1,2,3,4\n",
    "        if(key[0]>=4):\n",
    "            break\n",
    "    return ranked_list\n",
    "\n",
    "def clean_text(data: str)->str:\n",
    "    sub_space   = '[. \\- ,: /; \\[\\]\\(\\)=_]' #substitutes the special chars with space\n",
    "    data = data.replace(\"\\\\\", \" \") #replaces / with space\n",
    "    data = re.sub(sub_space,' ',data)\n",
    "    data = data.replace('\"',\"\")\n",
    "    data = data.replace(\"'\",\"\")  #replaces quotes with nothing\n",
    "    cleaned_data = data.encode('ascii',errors='ignore').decode()  #ignores non-ascii chars\n",
    "    return cleaned_data\n",
    "\n",
    "def tokenize_text(data: str):\n",
    "    word_data = word_tokenize(data)\n",
    "    token_list = []\n",
    "    for word in word_data:\n",
    "        #if(len(word)>1 and (word not in Stopwords)):\n",
    "        if(len(word)>1 or (word in ['a','i'])):\n",
    "            token_list.append(word.lower()) #tokenizing text.Including stop words\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def stem_text(token_data: []):\n",
    "    stemmed_tokens = []\n",
    "    for token in token_data:\n",
    "        stemmed_tokens.append(ps.stem(token))  #porter stemmer\n",
    "    return stemmed_tokens        \n",
    "\n",
    "def insert_unique_tokens(tokens: []):\n",
    "    for token in tokens:\n",
    "        vocabulary.add(token) #all the unique tokens\n",
    "    return vocabulary\n",
    "\n",
    "def preprocess(data: str):\n",
    "    cleaned_data   = clean_text(data)\n",
    "    token_data     = tokenize_text(cleaned_data)\n",
    "    stemmed_tokens = stem_text(token_data)\n",
    "    return stemmed_tokens\n",
    "\n",
    "\n",
    "def preprocess_query(query):\n",
    "    tokens = []\n",
    "    #if query is in string format\n",
    "    if(type(query) == str):\n",
    "        tokens = preprocess(query)\n",
    "        #if query is in list format\n",
    "    elif(type(query) == list):\n",
    "        for query_token in query:\n",
    "            preprocessed_tokens = preprocess(query_token)\n",
    "            for ptoken in preprocessed_tokens:\n",
    "                tokens.append(ptoken)\n",
    "    return tokens\n",
    "\n",
    "def write_results_to_file(ranked_list, file_name, query_id):\n",
    "   # print(file_name)\n",
    "   # print(ranked_list)\n",
    "    with open(file_name, 'a') as handle:\n",
    "        for doc_name in ranked_list:\n",
    "            file_line  = \"\"\n",
    "            file_line += query_id\n",
    "            file_line += \", \"\n",
    "            file_line += \"1, \"    #Iteration\n",
    "            file_line += doc_name #doc_id\n",
    "            file_line += \", \"\n",
    "            file_line += \"1\"    #relevance\n",
    "            handle.write(file_line)\n",
    "            handle.write('\\n')\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "def truncate_files(boolean_qrels, tfidf_qrels, bm25_qrels):\n",
    "    with open(boolean_qrels, \"w\") as fp:\n",
    "        fp.truncate(0)\n",
    "    with open(tfidf_qrels, \"w\") as fp:\n",
    "        fp.truncate(0)\n",
    "    with open(bm25_qrels, \"w\") as fp:\n",
    "        fp.truncate(0)\n",
    "        \n",
    "\n",
    "    \n",
    "def query(file_name):\n",
    "    #output files\n",
    "    truncate_files(\"QRels_Boolean.txt\",\"QRels_Tfidf.txt\", \"QRels_Bm25.txt\")\n",
    "    with open(file_name) as fp:\n",
    "        while(True):\n",
    "            line = fp.readline()\n",
    "            if((not line) or (len(line)==1)):\n",
    "                break\n",
    "            fields   = line.split(\"\\t\")\n",
    "            query_id = fields[0]\n",
    "            query    = fields[1].rstrip('\\n') #Removing newline if present at the end of query\n",
    "            \n",
    "            print('query_id {} query {}'.format(query_id, query))\n",
    "            \n",
    "            write_results_to_file(boolean_retrieval_query(query), \"QRels_Boolean.txt\", query_id)\n",
    "            write_results_to_file(tfidf_query(query), \"QRels_Tfidf.txt\", query_id)\n",
    "            write_results_to_file(bm25_query(query),  \"QRels_Bm25.txt\",query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6166f679",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_name is  21111011-qrels/queries.txt\n",
      "query_id Q01 query Deep learning in natural language processing (NLP)\n",
      "query_id Q02 query Hindu religion and their temples\n",
      "query_id Q03 query covid-19 pandemic vaccination\n",
      "query_id Q04 query data structures and algorithms in computer programming\n",
      "query_id Q05 query Infectious diseases prevention\n",
      "query_id Q06 query Kurukshetra war in Mahabharata\n",
      "query_id Q07 query literature, language and art\n",
      "query_id Q08 query unsupervised learning methods in machine learning\n",
      "query_id Q09 query quantum physics, quantum mechanics, quantum computing\n",
      "query_id Q10 query Rama Ravana war\n",
      "query_id Q11 query Object oriented computer programming\n",
      "query_id Q12 query Graph theory computer science\n",
      "query_id Q13 query human immune system\n",
      "query_id Q14 query unesco world heritage sites in India\n",
      "query_id Q15 query english fiction literature genre\n",
      "query_id Q16 query Figures of speech\n",
      "query_id Q17 query wildlife sanctuaries in India\n",
      "query_id Q18 query database management system concepts in computer science\n",
      "query_id Q19 query Np-hard np-complete problems\n",
      "query_id Q20 query Linux operating system\n"
     ]
    }
   ],
   "source": [
    "file_name = \"21111011-qrels/queries.txt\"\n",
    "print(\"file_name is \",file_name)\n",
    "query(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de35de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
